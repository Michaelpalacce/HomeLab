---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
    name: rook-ceph-cluster
    namespace: rook-ceph
spec:
    interval: 1h
    timeout: 15m
    install:
        createNamespace: true
        crds: CreateReplace
        remediation:
            retries: -1
    upgrade:
        cleanupOnFail: true
        remediation:
            retries: 3
    chart:
        spec:
            chart: rook-ceph-cluster
            version: v1.17.3
            interval: 1h
            sourceRef:
                kind: HelmRepository
                name: rook-release
                namespace: flux-system
    values:
        cephClusterSpec:
          cephVersion:
            image: quay.io/ceph/ceph:v19.2.2
          dataDirHostPath: /var/lib/rook
          mon:
            count: 3
            allowMultiplePerNode: false
            # enable the ceph dashboard for viewing cluster status
          dashboard:
            enabled: true
            # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
            urlPrefix: /
            # serve the dashboard at the given port.
            # port: 8443
            # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
            # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
            ssl: true
          # cluster level storage configuration and selection
          storage:
            useAllNodes: true
            useAllDevices: true
            # Only create OSDs on devices that match the regular expression filter, "sdb" in this example
            # @WARN: THIS WILL TAKE OVER `sdb` ENTIRELY, MAKE SURE IT'S RESERVED ONLY FOR CEPH
            deviceFilter: sdb
          # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
          # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=rook-node' and
          # the OSDs would specifically only be created on nodes labeled with 'role=rook-osd-node'.
          placement:
            all:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                      - rook-node
            osd:
              nodeAffinity:
                requiredDuringSchedulingIgnoredDuringExecution:
                  nodeSelectorTerms:
                  - matchExpressions:
                    - key: role
                      operator: In
                      values:
                      - rook-osd-node
        resources:
            prepareosd:
              requests:
                cpu: "200m"
                memory: "200Mi"
        priorityClassNames:
          # If there are multiple nodes available in a failure domain (e.g. zones), the
          # mons and osds can be portable and set the system-cluster-critical priority class.
          mon: system-node-critical
          osd: system-node-critical
          mgr: system-cluster-critical
        disruptionManagement:
          managePodBudgets: true
          osdMaintenanceTimeout: 30
